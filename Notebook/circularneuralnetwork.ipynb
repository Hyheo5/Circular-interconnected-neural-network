{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69e50cd-a360-48a0-8d9b-e3d6ce457a97",
   "metadata": {},
   "source": [
    "Title: Circular interconnected network: new type of neural network designed for Translation initiation site prediction  \n",
    "\n",
    "Readme:\n",
    "* TIS_dataset forder is assumed in the same directory of this notebook\n",
    "* python version: 3.8x / numpy version: 1.24.3 / \n",
    "* please refer the describing document. This notebook assume you read this document https://docs.google.com/document/d/1Z1xFTYC3RXYgH9YIxCmCeJJEO783fIz0KfJQoNL2dC0/edit?usp=sharing\n",
    "* This code is modified from    https://github.com/TheIndependentCode/Neural-Network\n",
    "    * Theoretical background and implementation of original version of this code is descrived in\n",
    "        * https://youtu.be/pauPCy_s0Ok?si=oYWBM5mMPHMc6vVf    (Overall code include Dense layer)\n",
    "        * https://youtu.be/Lakz2MoHy6o?si=JjQ39sk87yFRvuhm    (Convolutional layer)\n",
    "        * https://youtu.be/AbLvJVwySEo?si=zsEGSYAGg7HGAfM4    (Softmax layer)\n",
    "* Because of too long execution time, the models or hyperparameters are not optimized. Anyone who has willing might easily find improved version of each.\n",
    "\n",
    "This code will implement neural network as object and first use it for simplized mnist dataset, and to the DNA sequences dataset for translation initiation site prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303d818-e357-42a9-8690-162e75000b86",
   "metadata": {},
   "source": [
    "For describing the neural network,  \n",
    "Let  \n",
    "Y as output for an layer  \n",
    "X as input for an layer  \n",
    "W as weight for an layer  \n",
    "B as bias for an layer  \n",
    "E as Error for network(layers), which finally be calculated via error function  \n",
    "\n",
    "del_x/del_y: denote partial derivative of x by y  \n",
    "For each backpropagation, the code is assuming the del_E/del_Y is given by later(= previous in backward fassion) layer.  \n",
    "(the initial del_E/del_Y is calculated by taking derivative of error function)  \n",
    "And call this given del_E/del_Y as output_gradient.  \n",
    "\n",
    "Thus each layer should  \n",
    "1\\) return del_E/del_Y(= output_gradient) in backward methods to give it for next backward calculation.\n",
    " \n",
    "Also for optimizing parameter, it should calculate  \n",
    "2) del_E/del_W  \n",
    "3) del_E/del_B  \n",
    "and use this adjust weights and biases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f971deb-67d2-4b6e-98f3-bc6c0dfd5d29",
   "metadata": {},
   "source": [
    "This class will be the base class for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6005394-79c6-434c-a3fb-2c2dcb74712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Layer class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        #TODO: return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #TODO: 1)update parameters, 2) return input gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d1942-6b58-4012-a311-172f32d56a84",
   "metadata": {},
   "source": [
    "First build the Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda54996-422b-4107-9514-44e334d63048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c176410-75ec-41c7-80d9-6b4b1f5c9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer: Dense\n",
    "\n",
    "# let input size(= n_inputs) i\n",
    "# let output_size j\n",
    "\n",
    "#forward\n",
    "# input(= X) dimension: (i x 1), column vector\n",
    "# output(= Y) dimension: (j x 1), column vector\n",
    "# weight(= W) dimension: (j, i)\n",
    "# bias(= B) dimension: (j x 1), column vector\n",
    "\n",
    "# Y = W (dot) X + B ((dot) is dot product)\n",
    "\n",
    "#backward (given: del_E/del_Y as output_gradient, dimension: (j x 1))\n",
    "# del_E/del_W(dimension: (j, i)) = del_E/del_Y (dot) X_transpose \n",
    "# del_E/del_B(dimension: (j x 1), column vector) = del_E/del_Y \n",
    "# del_E/del_X(dimension: (j x 1), column vector) = W_transpose (dot) del_E/del_Y \n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size) # W : (j, i)\n",
    "        self.bias = np.random.randn(output_size, 1) # B : (j, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input # X : (i, 1)\n",
    "        return np.dot(self.weights, self.input) + self.bias # (j, i) (dot) (i, 1) + (j, 1)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T) # (j, i) = (j, 1) (dot) (1, i)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return np.dot(self.weights.T, output_gradient) # del_E/del_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86153e23-6e37-4839-abb2-a45a1812b3d7",
   "metadata": {},
   "source": [
    "Build Circular Layer, note that n_together is size of relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7840b673-5e97-43b2-9569-c44b84c76494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer: Circular\n",
    "\n",
    "# let input size(= n_inputs) i\n",
    "# let n_together t\n",
    "# then output_size will be i_C_t (i combination t), let this as j\n",
    "\n",
    "#forward\n",
    "# input(= X) dimension: (i x 1), column vector / this input X will makes combinations by n_together,\n",
    "# which I will call \"relations\", or X_comb as abbreviated.\n",
    "# relations(= X_comb) dimension: (j x 1), column vector\n",
    "# output(= Y) dimension: (j x 1), column vector\n",
    "# weight(= W) dimension: (j x 1), column vector\n",
    "# bias(= B) dimension: (j x 1), column vector\n",
    "# Y = W * X_comb + B (elementwise operation)\n",
    "\n",
    "\n",
    "#backward (given: del_E/del_Y as output_gradient, dimension: (j x 1))\n",
    "# del_E/del_W(dimension: (j x 1), column vector) = del_E/del_Y * X_comb (elementwise)\n",
    "# del_E/del_B(dimension: (j x 1), column vector) = del_E/del_Y \n",
    "# del_E/del_X(dimension: (i x 1), column vector) : (descrived at the documentation, take care of subscript of combitation form, not a inteiger)\n",
    "# let let k = (a combination), Each element del_E/del_xi in del_E/del_X \n",
    "# = sum(del_E/del_yk * X_comb[k_th element]) for k in all possible combinations, if k contains i\n",
    "\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "def product(iterable): # for forward method\n",
    "    result = 1\n",
    "    for x in iterable:\n",
    "        result = result*x \n",
    "    return result\n",
    "        \n",
    "\n",
    "class Circular(Layer):\n",
    "    def __init__(self, n_inputs, n_together=2):\n",
    "        self.n_together = n_together\n",
    "        self.len_weight = int(math.factorial(n_inputs) / (math.factorial(n_together)*math.factorial(n_inputs - n_together))) # self.len_weight = j\n",
    "        self.weights = np.random.randn(self.len_weight, 1) # W : (j, 1)\n",
    "        self.bias = np.random.randn(self.len_weight, 1) # B : (j, 1)\n",
    "        self.index = [i for i in combinations(range(n_inputs), n_together)]\n",
    "\n",
    "    def forward(self, input): # input(= X): (i x 1)\n",
    "        self.input = input \n",
    "        self.relations = np.array([product(i) for i in combinations(input, self.n_together)]).reshape(self.len_weight, 1) # X_comb: (j, 1)\n",
    "        return self.relations * self.weights + self.bias # (j, 1)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate): #output_gradient dimension: (j x 1)        \n",
    "        weights_gradient = output_gradient * self.relations # (j, 1)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient # (j, 1)\n",
    "        \n",
    "        input_gradient = []\n",
    "        for i in range(self.input.shape[0]): # for each input variable, for example x1, x2, ...\n",
    "            summed_term = 0\n",
    "            select_positions = [p for p, t in enumerate(self.index) if i in t] # select the position in which containing i\n",
    "            for j in select_positions: \n",
    "                summed_term += float(output_gradient[j] * self.relations[j])\n",
    "            input_gradient.append(summed_term)\n",
    "        input_gradient = np.array(input_gradient).reshape(self.input.shape[0], 1) # (i, 1)\n",
    "        return input_gradient\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b81d1d-c412-493e-b342-873c010b76b1",
   "metadata": {},
   "source": [
    "Build Convolutiona layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53272839-89bb-419a-840c-6f848c02a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer: Convolution\n",
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "        #print(self.output.shape)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
    "\n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62a920-5f75-40ff-9174-3b17b1b11059",
   "metadata": {},
   "source": [
    "This class will be the base for all activation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb2bb31-f880-4153-aa65-3db4ada07233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Activation layer class\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input): #input : (i, 1), output : (i, 1)\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: 1) update parameters(but no trainable parameters), 2) return input gradient\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de52eb-1e06-4595-a485-78754840b91d",
   "metadata": {},
   "source": [
    "Build several activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3523b7-526f-49fe-9690-7d07100c49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_prime = lambda x: 1 - np.tanh(x)**2\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        relu = lambda x: np.maximum(0, x)\n",
    "        relu_prime = lambda x: (x > 0).astype(float)\n",
    "        super().__init__(relu, relu_prime)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def safe_sigmoid(x):\n",
    "            # For preventing overflow, limit the input range\n",
    "            x_clipped = np.clip(x, -500, 500)\n",
    "            return 1 / (1 + np.exp(-x_clipped))\n",
    "\n",
    "        def safe_sigmoid_prime(x):\n",
    "            sigmoid_output = safe_sigmoid(x)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "        super().__init__(safe_sigmoid, safe_sigmoid_prime)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        # Subtract the max value from the input array for numerical stability\n",
    "        shift = input - np.max(input)\n",
    "        exps = np.exp(shift)\n",
    "        self.output = exps / np.sum(exps)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        # Adjusted backward computation with the safe forward output\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca47d28-1d28-45c0-a9a4-4ab89d738abd",
   "metadata": {},
   "source": [
    "Build Error functions, here mse and binary_cross_entropy was built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27cf6dfe-b6fb-48f7-a2bb-e5f603df333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Avoid division by zero and log(0) by clipping y_pred values\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    # Avoid division by zero by clipping y_pred values\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b97fa-833b-4dc9-aeb8-d834bb22dc59",
   "metadata": {},
   "source": [
    "Build predict loop for forward propagation and train loop for back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc55b6ab-18b7-46ba-9f53-541b34c2de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict, train\n",
    "def predict(network, input):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train, x_val=None, y_val=None, epochs = 1000, learning_rate = 0.01, verbose = True):\n",
    "    for e in range(epochs):\n",
    "        #train\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = predict(network, x)\n",
    "\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error = error / len(x_train)\n",
    "        if verbose:\n",
    "            if x_val is not None and len(x_val) > 0:\n",
    "                print(f\"{e + 1}/{epochs}, train error={error}\", end = ' ')\n",
    "            else:\n",
    "                print(f\"{e + 1}/{epochs}, train error={error}\")    \n",
    "                \n",
    "        #validation\n",
    "        if x_val is not None and len(x_val) > 0:            \n",
    "            error_val = 0\n",
    "            for x, y in zip(x_val, y_val):\n",
    "                # forward\n",
    "                output = predict(network, x)\n",
    "                # error\n",
    "                error_val += loss(y, output)\n",
    "            error_val = error_val / len(x_val)\n",
    "            if verbose:\n",
    "                print(f\"  validation error={error_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153dcd4-c949-4d4f-8523-74b07634fa91",
   "metadata": {},
   "source": [
    "Build reshaping layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa5c4df-eee9-45f9-ad21-0c3910e515ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501f0c2-c013-4e3c-b547-5259db2f7f13",
   "metadata": {},
   "source": [
    "For the fast check whether the implemented layer is working or not, prepare mnist dataset, the handwrited number image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb8bcd4b-f4b9-4ae1-9cfe-1e69360515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f97b6c-aaea-4090-9d88-f941cd6253a3",
   "metadata": {},
   "source": [
    "* version: np-utils                     0.6.0\n",
    "* version: keras                        2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c2a54d-4e66-49c3-9c9c-6f1c12b57e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections.abc import Iterable\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9af9f7-27e7-40c4-b370-e9061b420500",
   "metadata": {},
   "source": [
    "As it is simple check, exculde the data except for 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86576eab-78a1-456f-972d-e6e65b86e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    y = to_categorical(y)\n",
    "    y = y.reshape(len(y), 2, 1)\n",
    "    return x, y\n",
    "\n",
    "# load MNIST from server, limit to 100 images per class since we're not training on GPU\n",
    "(m_x_train, m_y_train), (m_x_test, m_y_test) = mnist.load_data()\n",
    "m_x_train, m_y_train = preprocess_data(m_x_train, m_y_train, 500)\n",
    "m_x_test, m_y_test = preprocess_data(m_x_test, m_y_test, 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bebc250-49ec-423a-b233-82d2751e19b0",
   "metadata": {},
   "source": [
    "Build convolutional model, to check whether error of train and validation is decrease as it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c330b3a-ed38-4d33-8064-9e82442a4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20, train error=0.39147647175344863   validation error=0.18406659619871465\n",
      "2/20, train error=0.12700877654532094   validation error=0.11721094763286864\n",
      "3/20, train error=0.08418077814935798   validation error=0.0869000686717636\n",
      "4/20, train error=0.06363767849222185   validation error=0.06924793848096711\n",
      "5/20, train error=0.05115577174635473   validation error=0.05844430175633433\n",
      "6/20, train error=0.0432034682809363   validation error=0.05098965058524624\n",
      "7/20, train error=0.03762562657276694   validation error=0.04607041820943734\n",
      "8/20, train error=0.03339814601677857   validation error=0.04250611061566109\n",
      "9/20, train error=0.029921225044868594   validation error=0.03938813119552638\n",
      "10/20, train error=0.02702313439888719   validation error=0.036327639407849416\n",
      "11/20, train error=0.024510698664234574   validation error=0.03374252108812848\n",
      "12/20, train error=0.022368460194230975   validation error=0.03232467059298107\n",
      "13/20, train error=0.020642817459206556   validation error=0.0318070368797415\n",
      "14/20, train error=0.019164894088832207   validation error=0.030781554294368116\n",
      "15/20, train error=0.017770037406417694   validation error=0.029026581793827\n",
      "16/20, train error=0.01643427460924949   validation error=0.02639370509278214\n",
      "17/20, train error=0.015188386592433364   validation error=0.023517675047077004\n",
      "18/20, train error=0.014123973748599968   validation error=0.021435635594391485\n",
      "19/20, train error=0.013265430020448403   validation error=0.01999174837555692\n",
      "20/20, train error=0.012541403184617248   validation error=0.018943795814077454\n",
      "Test, Right: 995 times, Wrong: 5 times, Wrong/Right = 0.005025125628140704\n"
     ]
    }
   ],
   "source": [
    "# Convolution network\n",
    "\n",
    "# neural network\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 100),\n",
    "    Sigmoid(),\n",
    "    Dense(100, 2),\n",
    "    Sigmoid()\n",
    "]\n",
    "\n",
    "# train\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    m_x_train,\n",
    "    m_y_train,\n",
    "    m_x_test,\n",
    "    m_y_test,\n",
    "    epochs=20,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(m_x_test, m_y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9062b2c-841a-4b7b-8f0c-eab05f8798ed",
   "metadata": {},
   "source": [
    "This seems good decrease in train and validation error, note that here the test set is equal to validation set, not a seperate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41822fce-c4cb-49fa-9dcb-f5b1a149c37d",
   "metadata": {},
   "source": [
    "Now build other model using Circular layer, to check whether Circular layer works or not. Here, because Circular layer itself might give output with too large size, used convolutional before it to reduce the input size into Circular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "483bffc0-6a43-4eac-8baf-f662a39e5c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train error=1.1619911340891247   validation error=0.3645011552236331\n",
      "2/10, train error=0.22568269522445436   validation error=0.31406259337417175\n",
      "3/10, train error=0.1538040279762966   validation error=0.27558185870328783\n",
      "4/10, train error=0.10572494692151056   validation error=0.22982245756546826\n",
      "5/10, train error=0.08359157008819565   validation error=0.19453750109677512\n",
      "6/10, train error=0.06734858519735075   validation error=0.18963391283293485\n",
      "7/10, train error=0.06181058752041263   validation error=0.16061836428355428\n",
      "8/10, train error=0.05132044872437344   validation error=0.10942657847583305\n",
      "9/10, train error=0.04232472020926567   validation error=0.09404775238975877\n",
      "10/10, train error=0.0364895351147966   validation error=0.08717474975947688\n",
      "Test, Right: 983 times, Wrong: 17 times, Wrong/Right = 0.017293997965412006\n"
     ]
    }
   ],
   "source": [
    "# Convolution network, with Circular Layer\n",
    "# Both model works actually, but I think there is no need to showing both output\n",
    "\n",
    "# neural network\n",
    "'''\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 30),\n",
    "    Sigmoid(),\n",
    "    Circular(30, 2), # n_together=2, thus output will be 30_C_2 = 435\n",
    "    ReLU(),\n",
    "    Dense(435, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "# train\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    m_x_train,\n",
    "    m_y_train,\n",
    "    m_x_test,\n",
    "    m_y_test,\n",
    "    epochs=20,\n",
    "    learning_rate=0.0012\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(m_x_test, m_y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")\n",
    "\n",
    "'''\n",
    "\n",
    "# Convolution network, with Circular Layer\n",
    "\n",
    "# neural network\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 30),\n",
    "    Sigmoid(),\n",
    "    Circular(30, 3), # n_together=3, thus output will be 30_C_3 = 4060\n",
    "    ReLU(),\n",
    "    Dense(4060, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "# train\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    m_x_train,\n",
    "    m_y_train,\n",
    "    m_x_test,\n",
    "    m_y_test,\n",
    "    epochs=10,\n",
    "    learning_rate=0.0006\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(m_x_test, m_y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe73861-c5b5-49fb-90f0-9a60d885658b",
   "metadata": {},
   "source": [
    "It seems works fine, test the layer again in more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0009245e-4f57-4aa8-87e7-fe351244514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train error=1.9343855213128818   validation error=1.6454316332799002\n",
      "2/10, train error=1.0560433361368684   validation error=0.5413971950768551\n",
      "3/10, train error=0.5557046178308283   validation error=0.45006505767932414\n",
      "4/10, train error=0.32075517788667846   validation error=0.3579076298304094\n",
      "5/10, train error=0.2713502388016024   validation error=0.609028501253306\n",
      "6/10, train error=0.23071739309064407   validation error=0.2287461649941782\n",
      "7/10, train error=0.2140869999916326   validation error=0.15705199304946862\n",
      "8/10, train error=0.13127163905656228   validation error=0.16164471725819587\n",
      "9/10, train error=0.14388122806349063   validation error=0.11859220105844318\n",
      "10/10, train error=0.09879278956102391   validation error=0.14094382491561072\n",
      "Test, Right: 975 times, Wrong: 25 times, Wrong/Right = 0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "# Convolution network, with Circular Layer\n",
    "\n",
    "# neural network\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 30),\n",
    "    Sigmoid(),\n",
    "    Circular(30, 2), # n_together=2, thus output will be 30_C_2 = 435\n",
    "    ReLU(),\n",
    "    Dense(435, 50),\n",
    "    ReLU(),\n",
    "    Dense(50, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "# train\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    m_x_train,\n",
    "    m_y_train,\n",
    "    m_x_test,\n",
    "    m_y_test,\n",
    "    epochs=10,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(m_x_test, m_y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99c8fe-9dab-4737-9691-aa4439b3d1dd",
   "metadata": {},
   "source": [
    "This also shows good decrease in train and validation error, note that again the test set is equal to validation set, not a seperate data.  \n",
    "\n",
    "  \n",
    "* Remark: This complex model may fail to learn in same hyperparameter, depend on its random initialization. I experienced more than half of trial gives fail to learning, giving about 17.xxx error for every epoch. It might be think that this circular layer model has high degree of freedom. Possible reason for that I can think is that each terms of output associated only just 2 input variable, as here I used size of relation(, which is n_together) as 2, adjusting parameter leads not as stable change as conventional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060deef0-e566-4391-bb26-f1d7366e0721",
   "metadata": {},
   "source": [
    "Thus layers are working properly, now apply it onto DNA dataset to predict translation initiation site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc76c04-4985-4516-a022-2fda6d79229c",
   "metadata": {},
   "source": [
    "Prepare TIS data,   \n",
    "arabTIS_train.pos file are DNA sequences containing TIS,   \n",
    "arabTIS_train.neg file are DNA sequences does not contain TIS   \n",
    "All the sequence length is identical as 300bp   \n",
    "\n",
    "Pos data will be labeled as True, Neg data will be labeled as False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5781f-9364-495b-9ed2-2ed6e44baaf2",
   "metadata": {},
   "source": [
    "Read each file,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabac828-7e98-47d1-8b38-cb2df9fdd650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequence at pos: 21342, sequence length: 300\n",
      "number of sequence at neg: 21342, sequence length: 300\n"
     ]
    }
   ],
   "source": [
    "f1 = open(\"TIS_dataset/arabTIS_train.pos\", 'r')\n",
    "f2 = open(\"TIS_dataset/arabTIS_train.neg\", 'r')\n",
    "pos_lines = f1.readlines()\n",
    "neg_lines = f2.readlines()\n",
    "for (idx, line) in enumerate(pos_lines):\n",
    "    pos_lines[idx] = line.strip()\n",
    "for (idx, line) in enumerate(neg_lines):\n",
    "    neg_lines[idx] = line.strip()\n",
    "f1.close()\n",
    "f2.close()\n",
    "print(f\"number of sequence at pos: {len(pos_lines)}, sequence length: {len(pos_lines[5])}\")\n",
    "print(f\"number of sequence at neg: {len(neg_lines)}, sequence length: {len(neg_lines[5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4247225-c4bd-4bfe-9216-f4cf37029550",
   "metadata": {},
   "source": [
    "Initialize the variables for one hot encode the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4bf65e6-e25b-4d8e-b8fe-231d7edd02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIS_pos.shape: (21342, 1, 300, 4), TIS_neg.shape: (21342, 1, 300, 4)\n"
     ]
    }
   ],
   "source": [
    "TIS_pos = np.zeros((len(pos_lines), 1, 300, 4))\n",
    "TIS_neg = np.zeros((len(neg_lines), 1, 300, 4))\n",
    "print(f\"TIS_pos.shape: {TIS_pos.shape}, TIS_neg.shape: {TIS_neg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e2950-e02f-4681-958f-7647f274a2c7",
   "metadata": {},
   "source": [
    "One hot encode the sequences,\n",
    "([0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0]) for (A, C, G, T, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40024867-4acc-4510-9a8f-0332d0e80d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21342, 1, 300, 4) (21342, 1, 300, 4)\n"
     ]
    }
   ],
   "source": [
    "for idx_0, i in enumerate(pos_lines):\n",
    "    for idx_1, j in enumerate(i):\n",
    "        if j == 'A':\n",
    "            TIS_pos[idx_0][0][idx_1] = np.array([0, 0, 0, 1])\n",
    "        elif j == 'C':\n",
    "            TIS_pos[idx_0][0][idx_1] = np.array([0, 0, 1, 0])\n",
    "        elif j == 'G':\n",
    "            TIS_pos[idx_0][0][idx_1] = np.array([0, 1, 0, 0])\n",
    "        elif j == 'T':\n",
    "            TIS_pos[idx_0][0][idx_1] = np.array([1, 0, 0, 0])\n",
    "        else:\n",
    "            TIS_pos[idx_0][0][idx_1] = np.array([0, 0, 0, 0])\n",
    "\n",
    "for idx_0, i in enumerate(neg_lines):\n",
    "    for idx_1, j in enumerate(i):\n",
    "        if j == 'A':\n",
    "            TIS_neg[idx_0][0][idx_1] = np.array([0, 0, 0, 1])\n",
    "        elif j == 'C':\n",
    "            TIS_neg[idx_0][0][idx_1] = np.array([0, 0, 1, 0])\n",
    "        elif j == 'G':\n",
    "            TIS_neg[idx_0][0][idx_1] = np.array([0, 1, 0, 0])\n",
    "        elif j == 'T':\n",
    "            TIS_neg[idx_0][0][idx_1] = np.array([1, 0, 0, 0])\n",
    "        else:\n",
    "            TIS_neg[idx_0][0][idx_1] = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(TIS_pos.shape, TIS_neg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b302076-88a1-4cad-9170-21eabbc7f967",
   "metadata": {},
   "source": [
    "Label the TIS positive data as true, [[1], [0]], a column vector,   \n",
    "Label the TIS negative data as False, [[0], [1]], a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b984f59-2612-4be7-89db-2470abbd83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIS_data = []\n",
    "for i in range(len(TIS_pos)):\n",
    "    TIS_data.append((TIS_pos[i], np.array([[1], [0]])))\n",
    "    TIS_data.append((TIS_neg[i], np.array([[0], [1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33012e-1a79-48ad-ae51-38133e4a61cf",
   "metadata": {},
   "source": [
    "Prepare the train, validation, test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "212ce75a-f9a8-4a7f-a5be-e4caf816ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data_index = random.sample(range(42684), 42684)\n",
    "val_data_index = []\n",
    "[val_data_index.append(train_data_index.pop()) for i in range(4000)]\n",
    "test_data_index = []\n",
    "[test_data_index.append(val_data_index.pop()) for i in range(3500)]\n",
    "train_data_index.sort()\n",
    "val_data_index.sort()\n",
    "test_data_index.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "104d92af-d497-4972-9a29-4b9b602e6f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: len(x_train): 38684, len(y_train): 38684\n",
      "validation: len(x_val): 500, len(y_val): 500\n",
      "test: len(x_test): 3500, len(y_test): 3500\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(42684):\n",
    "    if i in val_data_index:\n",
    "        x_val.append(TIS_data[i][0])\n",
    "        y_val.append(TIS_data[i][1])\n",
    "    elif i in test_data_index:\n",
    "        x_test.append(TIS_data[i][0])\n",
    "        y_test.append(TIS_data[i][1])\n",
    "    elif i in train_data_index:\n",
    "        x_train.append(TIS_data[i][0])\n",
    "        y_train.append(TIS_data[i][1])\n",
    "\n",
    "print('train:', f'len(x_train): {len(x_train)}, len(y_train): {len(y_train)}')\n",
    "print('validation:', f'len(x_val): {len(x_val)}, len(y_val): {len(y_val)}')\n",
    "print('test:', f'len(x_test): {len(x_test)}, len(y_test): {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52591d78-2915-4a46-adc4-d552ba867e75",
   "metadata": {},
   "source": [
    "The dataset is ready succesfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739f075-3b5e-4c69-b6c0-25f17afebbac",
   "metadata": {},
   "source": [
    "Reshape it into numpy array, enable numpy operation in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c87c7be-bbf3-4678-84b4-489e4cedaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c199f-468b-4b81-821e-fa282776e975",
   "metadata": {},
   "source": [
    "Check the dimension of prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd58ac0b-5aa8-4528-a65f-e38cb8a0571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5471b7d5-a06e-40d7-b89d-9b1ea3ec279f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38684, 2, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e135a48-4972-4dad-930b-ba52f7f378f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c1f8d-3e28-4c7c-84e6-daf68408aa96",
   "metadata": {},
   "source": [
    "Giving desired shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdc547-bc7d-4a20-8da0-620ed5f91aa2",
   "metadata": {},
   "source": [
    "Now make model containing the Circular layer, to see whether it can capture the features of TIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38b48f4f-cef7-4b87-8994-3cacf3f6b6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/80, train error=2.222203702719871   validation error=1.349319896815469\n",
      "2/80, train error=1.4118358523443713   validation error=1.0799821464037012\n",
      "3/80, train error=1.1439994298328755   validation error=0.9422419348575862\n",
      "4/80, train error=0.9918070375472801   validation error=0.8505113747710742\n",
      "5/80, train error=0.8959257211944895   validation error=0.7816558839202828\n",
      "6/80, train error=0.8284857606498162   validation error=0.73750038672399\n",
      "7/80, train error=0.777082605188581   validation error=0.7025743586482597\n",
      "8/80, train error=0.7351011298541124   validation error=0.6695665844946849\n",
      "9/80, train error=0.6994207235516217   validation error=0.642243930837283\n",
      "10/80, train error=0.6685529811028743   validation error=0.617401625719925\n",
      "11/80, train error=0.6410240146940213   validation error=0.5944500405382183\n",
      "12/80, train error=0.6161521564531883   validation error=0.5708929153891944\n",
      "13/80, train error=0.5933099369635566   validation error=0.5481784011537344\n",
      "14/80, train error=0.5725332597681059   validation error=0.5277256295177741\n",
      "15/80, train error=0.5533437825720112   validation error=0.5100296711830095\n",
      "16/80, train error=0.5356276256145566   validation error=0.49552896676635994\n",
      "17/80, train error=0.5191704403034949   validation error=0.4825753211214185\n",
      "18/80, train error=0.5038084943406289   validation error=0.4709743996026703\n",
      "19/80, train error=0.48929256580154806   validation error=0.46029326256447567\n",
      "20/80, train error=0.4756978420645978   validation error=0.4507782991762648\n",
      "21/80, train error=0.46290285959349475   validation error=0.441483970897218\n",
      "22/80, train error=0.4508900424671035   validation error=0.4329276685354494\n",
      "23/80, train error=0.4395241993935757   validation error=0.4247093969785935\n",
      "24/80, train error=0.4288311153725686   validation error=0.4170445473145372\n",
      "25/80, train error=0.41876155187246117   validation error=0.4101907036206123\n",
      "26/80, train error=0.4092419465637406   validation error=0.4040296376683851\n",
      "27/80, train error=0.4002001790179988   validation error=0.39768506484708555\n",
      "28/80, train error=0.3915933912180186   validation error=0.3918803001773672\n",
      "29/80, train error=0.3834359776689915   validation error=0.3868151901489225\n",
      "30/80, train error=0.3756603329476333   validation error=0.3815844860732323\n",
      "31/80, train error=0.3682119834740408   validation error=0.3770294700101213\n",
      "32/80, train error=0.3611254319926733   validation error=0.3728936345503695\n",
      "33/80, train error=0.3543403575299672   validation error=0.3690760060927524\n",
      "34/80, train error=0.34778242216493   validation error=0.3654842357081836\n",
      "35/80, train error=0.3415254775284385   validation error=0.3620323357692073\n",
      "36/80, train error=0.33552783908173417   validation error=0.35891898205116707\n",
      "37/80, train error=0.32975901699528587   validation error=0.3559566045109797\n",
      "38/80, train error=0.32424535933268495   validation error=0.3531832885438109\n",
      "39/80, train error=0.31892800630917106   validation error=0.3506009711730839\n",
      "40/80, train error=0.313820555985274   validation error=0.3480865041416138\n",
      "41/80, train error=0.3089166258983129   validation error=0.345564515929605\n",
      "42/80, train error=0.30420048892699847   validation error=0.343230879389341\n",
      "43/80, train error=0.299641321662994   validation error=0.34092776260398555\n",
      "44/80, train error=0.29523371020635725   validation error=0.3388760794855793\n",
      "45/80, train error=0.2909940825474103   validation error=0.33684570863025\n",
      "46/80, train error=0.28688100059764515   validation error=0.33494754395734866\n",
      "47/80, train error=0.2829009770307809   validation error=0.3329837870486865\n",
      "48/80, train error=0.2790702344892224   validation error=0.33108180206033005\n",
      "49/80, train error=0.2753643991107049   validation error=0.32924030428783324\n",
      "50/80, train error=0.27179714887132145   validation error=0.3271752742642481\n",
      "51/80, train error=0.26832548575485854   validation error=0.32537661438320603\n",
      "52/80, train error=0.2649601142490607   validation error=0.32347609621289103\n",
      "53/80, train error=0.26169467323111617   validation error=0.32182238509071603\n",
      "54/80, train error=0.25852372300716026   validation error=0.3200516479850345\n",
      "55/80, train error=0.25542586473432677   validation error=0.31848065149307186\n",
      "56/80, train error=0.25244304217880054   validation error=0.3168313786981334\n",
      "57/80, train error=0.24957448378703886   validation error=0.3151466685232754\n",
      "58/80, train error=0.24674201750059238   validation error=0.31376596249269106\n",
      "59/80, train error=0.24397813559267476   validation error=0.31202564366566066\n",
      "60/80, train error=0.24130354231452059   validation error=0.3104089068626639\n",
      "61/80, train error=0.23870287856317707   validation error=0.30899970414099925\n",
      "62/80, train error=0.2361594464231218   validation error=0.3074365514994274\n",
      "63/80, train error=0.2336889325184596   validation error=0.3059056963882985\n",
      "64/80, train error=0.23126655774120686   validation error=0.30449413403904796\n",
      "65/80, train error=0.2289075089641936   validation error=0.30317904372485277\n",
      "66/80, train error=0.2266184114041696   validation error=0.30174978845747114\n",
      "67/80, train error=0.22438315685904173   validation error=0.30051800922813643\n",
      "68/80, train error=0.222205286785224   validation error=0.29920383385501303\n",
      "69/80, train error=0.22006644133236755   validation error=0.29778231402551464\n",
      "70/80, train error=0.21798451265100677   validation error=0.29631535029165673\n",
      "71/80, train error=0.21593710980083616   validation error=0.29498557753670773\n",
      "72/80, train error=0.2139290584278162   validation error=0.2936598325136934\n",
      "73/80, train error=0.21196722749511804   validation error=0.2924185200337355\n",
      "74/80, train error=0.21004822982824167   validation error=0.2910983303733563\n",
      "75/80, train error=0.2081609879829077   validation error=0.28993005056295174\n",
      "76/80, train error=0.20630714330533134   validation error=0.288769206378449\n",
      "77/80, train error=0.20447778620031157   validation error=0.2875634115671287\n",
      "78/80, train error=0.20270105688397091   validation error=0.28651343122978895\n",
      "79/80, train error=0.20094919878030976   validation error=0.28546357855776155\n",
      "80/80, train error=0.1992360963511274   validation error=0.28440829920050903\n",
      "Test, Right: 3074 times, Wrong: 426 times, Wrong/Right = 0.13858165256994145\n"
     ]
    }
   ],
   "source": [
    "network = [\n",
    "    #Convolutional((1, 300, 4), 3, 4), # input shape (1, 300, 4) / kernel size 2 x 2 / number of kernel 4\n",
    "    #Sigmoid(),\n",
    "    Reshape((1, 300, 4), (1 * 300 * 4, 1)),\n",
    "    Dense(1 * 300 * 4, 50),\n",
    "    Sigmoid(),\n",
    "    Circular(50, 2), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(1225, 2),\n",
    "    Softmax()\n",
    "\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    epochs=80,\n",
    "    learning_rate=0.00055\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2364f-3130-4789-8191-cc46212ebcac",
   "metadata": {},
   "source": [
    "It shows decreasing at the error on both train and validation. Circular layer can capture the feature of TIS succesfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e648d-ed5f-426a-822f-22fba6e5daf1",
   "metadata": {},
   "source": [
    "Test circular layer again in more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2627afb-99ae-46ef-90c8-9b4d89584750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/80, train error=15.20988437295994   validation error=16.700661469511044\n",
      "2/80, train error=15.028111540875155   validation error=16.594196875464284\n",
      "3/80, train error=14.752672267052429   validation error=16.303972926180027\n",
      "4/80, train error=14.51329653374669   validation error=15.7364224104542\n",
      "5/80, train error=14.274532174888344   validation error=15.70931995059532\n",
      "6/80, train error=14.074482999459459   validation error=15.539106027408703\n",
      "7/80, train error=13.950851361692784   validation error=15.64453875407856\n",
      "8/80, train error=13.74583334793326   validation error=15.547577726369255\n",
      "9/80, train error=13.557855455436014   validation error=15.462629517532513\n",
      "10/80, train error=13.362731923319036   validation error=15.19895775268855\n",
      "11/80, train error=13.186683544380854   validation error=14.978312147379299\n",
      "12/80, train error=12.983959103740606   validation error=14.655859795517593\n",
      "13/80, train error=12.818832776451124   validation error=14.60804326347095\n",
      "14/80, train error=12.686489675113172   validation error=14.414066342941828\n",
      "15/80, train error=12.549940225025729   validation error=14.544832517706933\n",
      "16/80, train error=12.379378441958934   validation error=14.3332514372972\n",
      "17/80, train error=12.256797606294004   validation error=14.450297332274308\n",
      "18/80, train error=12.151212090742543   validation error=14.352214564350811\n",
      "19/80, train error=12.067011076861226   validation error=14.257670479341975\n",
      "20/80, train error=11.931819894897814   validation error=14.156459327871358\n",
      "21/80, train error=11.805601215679614   validation error=14.016014512858442\n",
      "22/80, train error=11.68923490646621   validation error=14.023324700681814\n",
      "23/80, train error=11.595371006159667   validation error=14.024853632306412\n",
      "24/80, train error=11.470232595579532   validation error=13.807889872081368\n",
      "25/80, train error=11.36309808988092   validation error=13.877472915190218\n",
      "26/80, train error=11.243349555596595   validation error=13.877465140992593\n",
      "27/80, train error=11.133096486622028   validation error=13.88853008396221\n",
      "28/80, train error=11.049592270429738   validation error=13.354565063727273\n",
      "29/80, train error=10.932276306670369   validation error=13.264744897623505\n",
      "30/80, train error=10.828882969479737   validation error=13.208351251327136\n",
      "31/80, train error=10.72889215609357   validation error=13.453913525005639\n",
      "32/80, train error=10.657744297578256   validation error=13.377919436032212\n",
      "33/80, train error=10.5551561618603   validation error=12.786669211712207\n",
      "34/80, train error=10.448134761271287   validation error=12.749089620343577\n",
      "35/80, train error=10.34676638900613   validation error=12.81380053275261\n",
      "36/80, train error=10.246763922047515   validation error=12.704187256521752\n",
      "37/80, train error=10.162064579805321   validation error=12.613439272599287\n",
      "38/80, train error=10.074944386858299   validation error=12.505513554347298\n",
      "39/80, train error=9.987118431829881   validation error=12.30412979264997\n",
      "40/80, train error=9.880071587465567   validation error=12.148597635440678\n",
      "41/80, train error=9.775287371586833   validation error=11.935597125886556\n",
      "42/80, train error=9.680537985876116   validation error=11.936155627544347\n",
      "43/80, train error=9.58785052628158   validation error=11.937423672147085\n",
      "44/80, train error=9.487223124127512   validation error=11.800585507268774\n",
      "45/80, train error=9.374459895127506   validation error=11.727362302556605\n",
      "46/80, train error=9.274718704209702   validation error=11.567723832639626\n",
      "47/80, train error=9.168265149432044   validation error=11.51274303046645\n",
      "48/80, train error=9.046307910402211   validation error=11.414110425722084\n",
      "49/80, train error=8.93981550038816   validation error=11.324759824066357\n",
      "50/80, train error=8.826142745144436   validation error=11.194550452326258\n",
      "51/80, train error=8.714470015825324   validation error=11.100541171123592\n",
      "52/80, train error=8.595267685237959   validation error=10.969928864245876\n",
      "53/80, train error=8.483896117582166   validation error=10.771912656337271\n",
      "54/80, train error=8.36838848316326   validation error=10.535070323884838\n",
      "55/80, train error=8.255127430592934   validation error=10.451744133038044\n",
      "56/80, train error=8.134907628002567   validation error=10.237560113278322\n",
      "57/80, train error=8.007857432947244   validation error=10.102758847914586\n",
      "58/80, train error=7.885420351511545   validation error=9.972337422433323\n",
      "59/80, train error=7.758671339217617   validation error=9.876501352425324\n",
      "60/80, train error=7.627904909959764   validation error=9.723328902054972\n",
      "61/80, train error=7.4919429816852015   validation error=9.49126605991767\n",
      "62/80, train error=7.353571530915108   validation error=9.314769669214574\n",
      "63/80, train error=7.218249567892881   validation error=9.089262300640556\n",
      "64/80, train error=7.077309760436207   validation error=8.91820377186871\n",
      "65/80, train error=6.935937300854598   validation error=8.700681502314803\n",
      "66/80, train error=6.787359045774988   validation error=8.506885789348418\n",
      "67/80, train error=6.64379357899033   validation error=8.334867404982292\n",
      "68/80, train error=6.502255046882583   validation error=8.158224286093573\n",
      "69/80, train error=6.358275225418059   validation error=7.9723476505212485\n",
      "70/80, train error=6.20905001591026   validation error=7.7705655043913495\n",
      "71/80, train error=6.062762415161862   validation error=7.599457735319772\n",
      "72/80, train error=5.913066789236394   validation error=7.428108615703684\n",
      "73/80, train error=5.765551093372798   validation error=7.282997315103655\n",
      "74/80, train error=5.6197070469326125   validation error=7.113494283011422\n",
      "75/80, train error=5.475340257566991   validation error=6.927749011161414\n",
      "76/80, train error=5.333721805703085   validation error=6.737902061133166\n",
      "77/80, train error=5.191268013175493   validation error=6.559656880813402\n",
      "78/80, train error=5.0514163192920085   validation error=6.391582812389433\n",
      "79/80, train error=4.915767364098704   validation error=6.19965470225057\n",
      "80/80, train error=4.778879513413012   validation error=6.011939239341901\n",
      "Test, Right: 1908 times, Wrong: 1592 times, Wrong/Right = 0.8343815513626834\n"
     ]
    }
   ],
   "source": [
    "network = [\n",
    "    #Convolutional((1, 300, 4), 3, 4), # input shape (1, 300, 4) / kernel size 2 x 2 / number of kernel 4\n",
    "    #Sigmoid(),\n",
    "    Reshape((1, 300, 4), (1 * 300 * 4, 1)),\n",
    "    Dense(1 * 300 * 4, 500),\n",
    "    ReLU(),\n",
    "    Dense(500, 50),\n",
    "    Sigmoid(),\n",
    "    Circular(50, 2), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(1225, 300),\n",
    "    ReLU(),\n",
    "    Dense(300, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    epochs=80,\n",
    "    learning_rate=0.000004\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19c5b6-21cc-41f8-b102-5a1613c04fd9",
   "metadata": {},
   "source": [
    "It seems the model is too complex for the dataset or need to find proper parameters, but it can decrease in its error function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79675cc8-ce99-43bd-b9b3-4cba4b5685e1",
   "metadata": {},
   "source": [
    "Remark: I tried to build the model that only uses conventional layers such as Dense and Convolution, for the compoarision with the model using Circular layer, but failed to build such model, giving all models no decrease of error at high value error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae4b6cb9-9131-4c23-a42c-595668be48ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train error=17.263584885506553   validation error=19.411017020817813\n",
      "2/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "3/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "4/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "5/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "6/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "7/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "8/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "9/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "10/10, train error=17.256195282143214   validation error=19.411017020817813\n",
      "Test, Right: 1766 times, Wrong: 1734 times, Wrong/Right = 0.9818799546998868\n"
     ]
    }
   ],
   "source": [
    "#control model\n",
    "network = [\n",
    "    #Convolutional((1, 300, 4), 3, 4), # input shape (1, 300, 4) / kernel size 2 x 2 / number of kernel 4\n",
    "    #Sigmoid(),\n",
    "    Reshape((1, 300, 4), (1 * 300 * 4, 1)),\n",
    "    Dense(1 * 300 * 4, 500),\n",
    "    ReLU(),\n",
    "    Dense(500, 200), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(200, 50), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(50, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    epochs=10,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "842e6914-2707-4aaf-a395-ff4ae19e5442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train error=17.27903915087038   validation error=15.128159172808166\n",
      "2/10, train error=17.268537311153477   validation error=19.480095373205067\n",
      "3/10, train error=17.252535362731177   validation error=19.411017020817813\n",
      "4/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "5/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "6/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "7/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "8/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "9/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "10/10, train error=17.254409573519823   validation error=19.411017020817813\n",
      "Test, Right: 1767 times, Wrong: 1733 times, Wrong/Right = 0.9807583474816073\n"
     ]
    }
   ],
   "source": [
    "# control model\n",
    "network = [\n",
    "    Convolutional((1, 300, 4), 4, 4), # input shape (1, 300, 4) / kernel size 2 x 2 / number of kernel 4\n",
    "    Sigmoid(),\n",
    "    Reshape((4, 297, 1), (4 * 297 * 1, 1)),\n",
    "    Dense(4 * 297 * 1, 200),\n",
    "    ReLU(),\n",
    "    Dense(200, 50), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(50, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    epochs=10,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79aacc37-0bf9-48cc-b1c4-a1254cb3c5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "2/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "3/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "4/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "5/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "6/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "7/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "8/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "9/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "10/10, train error=17.290123745987664   validation error=15.128159172808166\n",
      "Test, Right: 1733 times, Wrong: 1767 times, Wrong/Right = 1.0196191575302942\n"
     ]
    }
   ],
   "source": [
    "#control model\n",
    "network = [\n",
    "    #Convolutional((1, 300, 4), 3, 4), # input shape (1, 300, 4) / kernel size 2 x 2 / number of kernel 4\n",
    "    #Sigmoid(),\n",
    "    Reshape((1, 300, 4), (1 * 300 * 4, 1)),\n",
    "    Dense(1 * 300 * 4, 500),\n",
    "    ReLU(),\n",
    "    Dense(500, 500), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(500, 200), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(200, 200), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(200, 50), # n_together=2, thus output will be 50_C_2\n",
    "    ReLU(),\n",
    "    Dense(50, 2),\n",
    "    Softmax()\n",
    "\n",
    "]\n",
    "\n",
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    epochs=10,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "\n",
    "# test\n",
    "Right = 0\n",
    "Wrong = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        Right += 1\n",
    "    else:\n",
    "        Wrong += 1\n",
    "print(f\"Test, Right: {Right} times, Wrong: {Wrong} times, Wrong/Right = {Wrong/Right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68417bd4-df23-4087-8085-3258c3b56ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
